{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirajkrgupta/classical-models/blob/main/crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4d6w8Foaq2C9",
        "outputId": "f95b4a23-8814-4d0a-85ed-8e727cea297c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.10-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (4.67.1)\n",
            "Downloading textstat-0.7.10-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.17.2 textstat-0.7.10\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat\n",
        "!pip install gdown  # only if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2vRpZzQdP1r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score,roc_curve,auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ZWpSpIx0_-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwzW9fL_rHHH"
      },
      "source": [
        "# **Step 1: Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDxMs--NirR4"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data(json_file_path):\n",
        "    \"\"\"\n",
        "    Load the dataset and create labels for human vs AI classification\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    # Read the JSON dataset\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping line {line_num} due to JSONDecodeError: {e}\")\n",
        "                continue  # Skip to the next line if decoding fails\n",
        "    #make a table from the data\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Create the classification dataset\n",
        "    classification_data = []\n",
        "    #iterate through each row of the table df\n",
        "    for _, row in df.iterrows():\n",
        "        # Human review (label = 1)\n",
        "        human_review = row.get('full_human_review', '')\n",
        "        if human_review and str(human_review).strip():  # Check if not empty\n",
        "            classification_data.append({\n",
        "                'text': str(human_review).strip(),\n",
        "                'label': 1,  # Human = 1\n",
        "                'source': 'human'\n",
        "            })\n",
        "\n",
        "        # AI/Machine review (label = 0)\n",
        "        machine_review = row.get('machine_review', '')\n",
        "        if machine_review and str(machine_review).strip():  # Check if not empty\n",
        "            classification_data.append({\n",
        "                'text': str(machine_review).strip(),\n",
        "                'label': 0,  # AI = 0\n",
        "                'source': 'ai'\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(classification_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwlxgNpr6sW"
      },
      "source": [
        "# **Step 2: Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nyx9NhPr5NZ"
      },
      "outputs": [],
      "source": [
        "class ReviewFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extract various features from review texts\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize sentiment analyzer\n",
        "        nltk.download('vader_lexicon', quiet=True)\n",
        "        self.sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def extract_linguistic_features(self, text):\n",
        "        \"\"\"Extract linguistic and stylistic features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Handle empty, None, or invalid text\n",
        "        if not text or not isinstance(text, str) or pd.isna(text):\n",
        "            text = \"\"\n",
        "\n",
        "        text = str(text).strip()\n",
        "\n",
        "        # Basic text statistics\n",
        "        features['char_count'] = len(text)\n",
        "        words = text.split() if text else []\n",
        "        features['word_count'] = len(words)\n",
        "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()] if text else []\n",
        "        features['sentence_count'] = max(len(sentences), 1)\n",
        "        features['avg_word_length'] = np.mean([len(word) for word in words]) if words else 0\n",
        "        features['avg_sentence_length'] = features['word_count'] / features['sentence_count']\n",
        "\n",
        "        # Replace NaN values with 0\n",
        "        if np.isnan(features['avg_word_length']):\n",
        "            features['avg_word_length'] = 0\n",
        "\n",
        "        # Readability scores (with error handling)\n",
        "        try:\n",
        "            if text and len(text) > 10:\n",
        "                features['flesch_reading_ease'] = flesch_reading_ease(text)\n",
        "                features['flesch_kincaid_grade'] = flesch_kincaid_grade(text)\n",
        "            else:\n",
        "                features['flesch_reading_ease'] = 0\n",
        "                features['flesch_kincaid_grade'] = 0\n",
        "        except:\n",
        "            features['flesch_reading_ease'] = 0\n",
        "            features['flesch_kincaid_grade'] = 0\n",
        "\n",
        "        # Punctuation and formatting\n",
        "        text_len = max(len(text), 1)\n",
        "        features['exclamation_ratio'] = text.count('!') / text_len\n",
        "        features['question_ratio'] = text.count('?') / text_len\n",
        "        features['comma_ratio'] = text.count(',') / text_len\n",
        "        features['bullet_points'] = text.count('*') + text.count('-')\n",
        "\n",
        "        # Sentiment analysis\n",
        "        try:\n",
        "            if text:\n",
        "                sentiment_scores = self.sia.polarity_scores(text)\n",
        "                features['sentiment_pos'] = sentiment_scores['pos']\n",
        "                features['sentiment_neg'] = sentiment_scores['neg']\n",
        "                features['sentiment_neu'] = sentiment_scores['neu']\n",
        "                features['sentiment_compound'] = sentiment_scores['compound']\n",
        "            else:\n",
        "                features['sentiment_pos'] = 0\n",
        "                features['sentiment_neg'] = 0\n",
        "                features['sentiment_neu'] = 0.5\n",
        "                features['sentiment_compound'] = 0\n",
        "        except:\n",
        "            features['sentiment_pos'] = 0\n",
        "            features['sentiment_neg'] = 0\n",
        "            features['sentiment_neu'] = 0.5\n",
        "            features['sentiment_compound'] = 0\n",
        "\n",
        "        # Academic writing indicators\n",
        "        if text:\n",
        "            features['first_person_pronouns'] = len(re.findall(r'\\b(I|me|my|we|us|our)\\b', text, re.IGNORECASE))\n",
        "            features['passive_voice_indicators'] = len(re.findall(r'\\b(is|are|was|were|been|being)\\s+\\w+ed\\b', text))\n",
        "            features['academic_phrases'] = len(re.findall(r'\\b(however|furthermore|moreover|therefore|consequently|nevertheless)\\b', text, re.IGNORECASE))\n",
        "        else:\n",
        "            features['first_person_pronouns'] = 0\n",
        "            features['passive_voice_indicators'] = 0\n",
        "            features['academic_phrases'] = 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_all_features(self, texts):\n",
        "        \"\"\"Extract features for all texts\"\"\"\n",
        "        features_list = []\n",
        "        for text in texts:\n",
        "            try:\n",
        "                features = self.extract_linguistic_features(str(text))\n",
        "                features_list.append(features)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing text: {e}\")\n",
        "                # Create default features in case of error\n",
        "                features = {\n",
        "                    'char_count': 0, 'word_count': 0, 'sentence_count': 1,\n",
        "                    'avg_word_length': 0, 'avg_sentence_length': 0,\n",
        "                    'flesch_reading_ease': 0, 'flesch_kincaid_grade': 0,\n",
        "                    'exclamation_ratio': 0, 'question_ratio': 0, 'comma_ratio': 0,\n",
        "                    'bullet_points': 0, 'sentiment_pos': 0, 'sentiment_neg': 0,\n",
        "                    'sentiment_neu': 0, 'sentiment_compound': 0,\n",
        "                    'first_person_pronouns': 0, 'passive_voice_indicators': 0,\n",
        "                    'academic_phrases': 0\n",
        "                }\n",
        "                features_list.append(features)\n",
        "\n",
        "        return pd.DataFrame(features_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0CAm13nsEIs"
      },
      "source": [
        "# **Step 3: Text Vectorization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-StvcgJisNr1"
      },
      "outputs": [],
      "source": [
        "def create_text_features(texts, max_features=5000, ngram_range=(1, 2), vectorizer_type='tfidf'):\n",
        "    \"\"\"\n",
        "    Create text features using different vectorization methods\n",
        "    \"\"\"\n",
        "    # Clean and preprocess texts\n",
        "    cleaned_texts = []\n",
        "    for text in texts:\n",
        "        if text and isinstance(text, str) and not pd.isna(text):\n",
        "            cleaned_texts.append(str(text).strip())\n",
        "        else:\n",
        "            cleaned_texts.append(\"\")\n",
        "\n",
        "    # Choose vectorizer\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            ngram_range=ngram_range,\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            strip_accents='ascii',\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        )\n",
        "    elif vectorizer_type == 'count':\n",
        "        vectorizer = CountVectorizer(\n",
        "            max_features=max_features,\n",
        "            ngram_range=ngram_range,\n",
        "            stop_words='english',\n",
        "            lowercase=True,\n",
        "            strip_accents='ascii',\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "    text_features = vectorizer.fit_transform(cleaned_texts)\n",
        "    return text_features, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeqG8ysIsZIh"
      },
      "source": [
        "# **Step 4: Model Training Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWzrUGNMsb8y"
      },
      "outputs": [],
      "source": [
        "class FlexibleAIHumanClassifier:\n",
        "    \"\"\"\n",
        "    Flexible classifier that can use any sklearn classifier with cross-validation training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classifier_name='logistic', max_features=3000, ngram_range=(1, 2),\n",
        "                 vectorizer_type='tfidf', use_scaling=True, cv_folds=5):\n",
        "        self.feature_extractor = ReviewFeatureExtractor()\n",
        "        self.vectorizer = None\n",
        "        self.scaler = StandardScaler() if use_scaling else None\n",
        "        self.use_scaling = use_scaling\n",
        "        self.max_features = max_features\n",
        "        self.ngram_range = ngram_range\n",
        "        self.vectorizer_type = vectorizer_type\n",
        "        self.classifier_name = classifier_name\n",
        "        self.cv_folds = cv_folds\n",
        "        self.cv_results = None\n",
        "\n",
        "        # Initialize the classifier based on name\n",
        "        self.model = self._get_classifier(classifier_name)\n",
        "\n",
        "    def _get_classifier(self, classifier_name):\n",
        "        \"\"\"Get classifier by name with optimized parameters\"\"\"\n",
        "        classifiers = {\n",
        "            'logistic': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "            'svm_linear': LinearSVC(C=1.0,max_iter=1000,tol=1e-3,dual=False,random_state=4),\n",
        "            'naive_bayes_multinomial': MultinomialNB(),\n",
        "            'knn': KNeighborsClassifier(n_neighbors=5),\n",
        "            'decision_tree': DecisionTreeClassifier(random_state=42),\n",
        "            'extra_trees': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "            'lda': LinearDiscriminantAnalysis(),\n",
        "            'xgboost': XGBClassifier(random_state=42, n_jobs=-1),\n",
        "            'mlp': MLPClassifier(random_state=42)\n",
        "        }\n",
        "\n",
        "        if classifier_name not in classifiers:\n",
        "            raise ValueError(f\"Classifier '{classifier_name}' not supported. Available: {list(classifiers.keys())}\")\n",
        "\n",
        "        return classifiers[classifier_name]\n",
        "\n",
        "    def prepare_features(self, texts, fit_transform=True):\n",
        "        \"\"\"Prepare combined features (linguistic + text)\"\"\"\n",
        "        # Extract linguistic features\n",
        "        linguistic_features = self.feature_extractor.extract_all_features(texts)\n",
        "\n",
        "        # Extract text features\n",
        "        if fit_transform:\n",
        "            text_features, self.vectorizer = create_text_features(\n",
        "                texts, self.max_features, self.ngram_range, self.vectorizer_type\n",
        "            )\n",
        "        else:\n",
        "            text_features = self.vectorizer.transform(texts)\n",
        "\n",
        "        # Scale linguistic features if needed, but not for MultinomialNB\n",
        "        if self.use_scaling and self.scaler and self.classifier_name != 'naive_bayes_multinomial':\n",
        "            linguistic_features_scaled = self.scaler.fit_transform(linguistic_features.values) if fit_transform else self.scaler.transform(linguistic_features.values)\n",
        "        else:\n",
        "            linguistic_features_scaled = linguistic_features.values\n",
        "\n",
        "        # Handle different classifiers that need dense vs sparse matrices\n",
        "        if self.classifier_name == 'naive_bayes_multinomial':\n",
        "            combined_features = text_features\n",
        "        elif self.classifier_name == 'svm_linear' and not self.use_scaling:\n",
        "            # Keep everything sparse\n",
        "            linguistic_sparse = sparse.csr_matrix(linguistic_features_scaled)\n",
        "            combined_features = sparse.hstack([linguistic_sparse, text_features])\n",
        "        else:\n",
        "            # For classifiers that require dense input\n",
        "            text_dense = text_features.toarray()\n",
        "            combined_features = np.hstack([linguistic_features_scaled, text_dense])\n",
        "\n",
        "        return combined_features\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"Train the classifier with cross-validation\"\"\"\n",
        "        # Prepare features\n",
        "        X_train_features = self.prepare_features(X_train, fit_transform=True)\n",
        "\n",
        "        # Define scoring metrics\n",
        "        scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "        # Perform cross-validation to evaluate performance\n",
        "        self.cv_results = cross_validate(\n",
        "            self.model, X_train_features, y_train,\n",
        "            cv=self.cv_folds, scoring=scoring,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model on the entire training set\n",
        "        self.model.fit(X_train_features, y_train)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_cv_results(self):\n",
        "        \"\"\"Get cross-validation results\"\"\"\n",
        "        if self.cv_results is None:\n",
        "            return None\n",
        "\n",
        "        results_summary = {}\n",
        "        for metric in ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']:\n",
        "            scores = self.cv_results[metric]\n",
        "            results_summary[metric] = {\n",
        "                'mean': scores.mean(),\n",
        "                'std': scores.std(),\n",
        "                'scores': scores\n",
        "            }\n",
        "        return results_summary\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        # Prepare features\n",
        "        X_test_features = self.prepare_features(X_test, fit_transform=False)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.model.predict(X_test_features)\n",
        "\n",
        "        # Get probabilities if available\n",
        "        try:\n",
        "            probabilities = self.model.predict_proba(X_test_features)\n",
        "        except:\n",
        "            probabilities = None\n",
        "\n",
        "        return predictions, probabilities\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluate the model\"\"\"\n",
        "        predictions, probabilities = self.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "        report = classification_report(y_test, predictions, target_names=['AI', 'Human'])\n",
        "        conf_matrix = confusion_matrix(y_test, predictions)\n",
        "        precision = precision_score(y_test, predictions)\n",
        "        recall = recall_score(y_test, predictions)\n",
        "        f1 = f1_score(y_test, predictions)\n",
        "\n",
        "        # Calculate AUC if probabilities are available\n",
        "        auc_score = None\n",
        "        if probabilities is not None:\n",
        "            auc_score = roc_auc_score(y_test, probabilities[:, 1])\n",
        "            # Plot ROC curve\n",
        "            fpr, tpr, thresholds = roc_curve(y_test, probabilities[:, 1])\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title('Receiver Operating Characteristic (ROC)')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.show()\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'auc_score': auc_score,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'classification_report': report,\n",
        "            'confusion_matrix': conf_matrix,\n",
        "            'predictions': predictions,\n",
        "            'probabilities': probabilities\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wp0iI4wtBSm"
      },
      "source": [
        "# **Step 6: Usage Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtQqu9nztD4B"
      },
      "outputs": [],
      "source": [
        "df = load_and_prepare_data('/content/subtaskC_train_dev.jsonl')\n",
        "X = df['text'].values\n",
        "y = df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXhMDPEsqGsR",
        "outputId": "77554e6c-00e1-42cb-ea8b-995c4b3c7fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38906\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuqD2ppfG3H"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8kk70pbptYSZ",
        "outputId": "f0eeee43-66d0-4376-e0cc-8d1f6f675747"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7961c48e3560>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = FlexibleAIHumanClassifier(classifier_name='logistic', cv_folds=5)\n",
        "classifier.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70z5dAf_UnM9",
        "outputId": "95fdac5e-27b4-4e4d-cb33-17c9e11347c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9588 +/- 0.0055\n",
            "test_precision: 0.9602 +/- 0.0072\n",
            "test_recall: 0.9573 +/- 0.0066\n",
            "test_f1: 0.9587 +/- 0.0055\n"
          ]
        }
      ],
      "source": [
        "cv_results = classifier.get_cv_results()\n",
        "for metric, values in cv_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUEtK7Gc9urz",
        "outputId": "fa839a5b-520e-4418-8c44-2e8bbf10364b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7c9789397740>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logistic10 = FlexibleAIHumanClassifier(classifier_name='logistic', cv_folds=10)\n",
        "logistic10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnNksNtq9vPZ",
        "outputId": "d9029b57-eb7f-49d5-c556-c15eff5a8879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9685 +/- 0.0326\n",
            "test_precision: 0.9556 +/- 0.0532\n",
            "test_recall: 0.9871 +/- 0.0110\n",
            "test_f1: 0.9702 +/- 0.0284\n"
          ]
        }
      ],
      "source": [
        "cv_results10 = logistic10.get_cv_results()\n",
        "for metric, values in cv_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmhsjMdpf8M7"
      },
      "source": [
        "**MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jf2ktVBgH5R",
        "outputId": "8e85a566-2c7f-4ac2-fbb4-914ac5aee097"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e642a04f740>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp = FlexibleAIHumanClassifier(classifier_name='mlp', cv_folds=5)\n",
        "mlp.train(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Hd87BMVv0X",
        "outputId": "f685976f-6d8a-48d8-89d5-e37a84ff19c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9788 +/- 0.0131\n",
            "test_precision: 0.9728 +/- 0.0169\n",
            "test_recall: 0.9858 +/- 0.0283\n",
            "test_f1: 0.9789 +/- 0.0133\n"
          ]
        }
      ],
      "source": [
        "mlp_cv_results = mlp.get_cv_results()\n",
        "for metric, values in mlp_cv_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnYXSKx_-IRS",
        "outputId": "4cd060f7-f441-432d-9853-ed8c19f52f96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7efbdd5cbf50>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp10 = FlexibleAIHumanClassifier(classifier_name='mlp', cv_folds=10)\n",
        "mlp10.train(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hLag3sH-Ito",
        "outputId": "67f966d6-a7df-4fe1-ae73-f167d68c2ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9874 +/- 0.0138\n",
            "test_precision: 0.9784 +/- 0.0248\n",
            "test_recall: 0.9977 +/- 0.0067\n",
            "test_f1: 0.9877 +/- 0.0130\n"
          ]
        }
      ],
      "source": [
        "mlp_cv_results10 = mlp10.get_cv_results()\n",
        "for metric, values in mlp_cv_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E75MvtMKhEs9"
      },
      "source": [
        "**Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRCvmxinhK2j",
        "outputId": "025e17e3-099d-4f4e-b04c-7e07e058a7bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e64263f5910>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_forest = FlexibleAIHumanClassifier(classifier_name='random_forest', cv_folds=5)\n",
        "random_forest.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aljIbKGiWI-o",
        "outputId": "75ddf02d-c7af-4c43-8411-54c072c47e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9714 +/- 0.0442\n",
            "test_precision: 0.9889 +/- 0.0139\n",
            "test_recall: 0.9543 +/- 0.0910\n",
            "test_f1: 0.9687 +/- 0.0498\n"
          ]
        }
      ],
      "source": [
        "random_forest_cv_results = random_forest.get_cv_results()\n",
        "for metric, values in random_forest_cv_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0EOGcc2-ZE0",
        "outputId": "a1584b17-09fb-4522-fe17-9fdad1fa09cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7efbdd5cbd40>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_forest10 = FlexibleAIHumanClassifier(classifier_name='random_forest', cv_folds=10)\n",
        "random_forest10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Pc0OPU-cV7",
        "outputId": "8c783bb2-41c8-4a1d-f018-afc4125a3311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9926 +/- 0.0113\n",
            "test_precision: 0.9911 +/- 0.0175\n",
            "test_recall: 0.9946 +/- 0.0153\n",
            "test_f1: 0.9927 +/- 0.0112\n"
          ]
        }
      ],
      "source": [
        "random_forest_cv_results10 = random_forest10.get_cv_results()\n",
        "for metric, values in random_forest_cv_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6qIpjQohxqt"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3FCGtM8zheQM",
        "outputId": "d6b716a5-e01a-44dc-808b-b19036d6587a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e64253c2570>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_bayes = FlexibleAIHumanClassifier(classifier_name='naive_bayes_multinomial', cv_folds=5)\n",
        "naive_bayes.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aewzTMTdWwCB",
        "outputId": "a71fbf15-4c80-4954-a6d2-486da8d610ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9115 +/- 0.0352\n",
            "test_precision: 0.8632 +/- 0.0507\n",
            "test_recall: 0.9839 +/- 0.0044\n",
            "test_f1: 0.9187 +/- 0.0288\n"
          ]
        }
      ],
      "source": [
        "naive_bayes_cv_results = naive_bayes.get_cv_results()\n",
        "for metric, values in naive_bayes_cv_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3jPI5Qx-n2p",
        "outputId": "32042e27-40e5-444d-9269-5adaae7aa4a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7c97886af920>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_bayes10 = FlexibleAIHumanClassifier(classifier_name='naive_bayes_multinomial', cv_folds=10)\n",
        "naive_bayes10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUELRJpj-p3A",
        "outputId": "0da1277a-c7ea-424e-a3ba-6ee21268b326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9167 +/- 0.0481\n",
            "test_precision: 0.8724 +/- 0.0645\n",
            "test_recall: 0.9856 +/- 0.0033\n",
            "test_f1: 0.9241 +/- 0.0380\n"
          ]
        }
      ],
      "source": [
        "naive_bayes_cv_results10 = naive_bayes10.get_cv_results()\n",
        "for metric, values in naive_bayes_cv_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJE-6SnsiPIl"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1UdoG1YKiKQ4",
        "outputId": "c1161b5e-ab5f-4e97-da01-20c67ecd6160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e6424def8f0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "knn = FlexibleAIHumanClassifier(classifier_name='knn', cv_folds=5)\n",
        "knn.train(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trILI5C9p6kC",
        "outputId": "ec53d4ab-92a3-4068-f414-678722ce3400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4304\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKIGNdejW8ZK",
        "outputId": "8dd7d78c-a82b-4c7c-9580-f2baaed7bf0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9008 +/- 0.0436\n",
            "test_precision: 0.8639 +/- 0.0326\n",
            "test_recall: 0.9543 +/- 0.0888\n",
            "test_f1: 0.9044 +/- 0.0474\n"
          ]
        }
      ],
      "source": [
        "knn_cv_results = knn.get_cv_results()\n",
        "for metric, values in knn_cv_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YQl-I4uV-86E",
        "outputId": "2935e2ab-c650-42e2-ea17-7bc698ad96e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7efbcdb51190>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "knn10= FlexibleAIHumanClassifier(classifier_name='knn', cv_folds=10)\n",
        "knn10.train(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JN5vTfED-9SR",
        "outputId": "4577a165-46db-41f3-fe88-4bc859d5bb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9186 +/- 0.0458\n",
            "test_precision: 0.8803 +/- 0.0407\n",
            "test_recall: 0.9723 +/- 0.0823\n",
            "test_f1: 0.9217 +/- 0.0489\n"
          ]
        }
      ],
      "source": [
        "knn_cv_results10 = knn10.get_cv_results()\n",
        "for metric, values in knn_cv_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcPNWAa4iv2T"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQBiQQSGi1Ug",
        "outputId": "30cca33b-9f16-44db-ec34-a529397df239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e6424f72210>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "desicion_tree = FlexibleAIHumanClassifier(classifier_name='decision_tree', cv_folds=5)\n",
        "desicion_tree.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsQn_TYqXMpG",
        "outputId": "6168dd28-56e9-4bef-d5df-b1b47c688885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9443 +/- 0.0447\n",
            "test_precision: 0.9378 +/- 0.0261\n",
            "test_recall: 0.9535 +/- 0.0926\n",
            "test_f1: 0.9428 +/- 0.0501\n"
          ]
        }
      ],
      "source": [
        "desicion_tree_results = desicion_tree.get_cv_results()\n",
        "for metric, values in desicion_tree_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cAgD-OAj_H-j",
        "outputId": "c76af733-9ccc-4f77-8dae-51741fcb235f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7efbcf3977d0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "desicion_tree10 = FlexibleAIHumanClassifier(classifier_name='decision_tree', cv_folds=10)\n",
        "desicion_tree10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QCBPQzaC_IYG",
        "outputId": "9dabe241-435c-4836-c944-b8013708bd54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9695 +/- 0.0185\n",
            "test_precision: 0.9480 +/- 0.0306\n",
            "test_recall: 0.9951 +/- 0.0141\n",
            "test_f1: 0.9706 +/- 0.0169\n"
          ]
        }
      ],
      "source": [
        "desicion_tree_results10 = desicion_tree10.get_cv_results()\n",
        "for metric, values in desicion_tree_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrDQfCF8i6Ed"
      },
      "source": [
        "**Extra Trees**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QRr-1vbi7nK",
        "outputId": "64cdd701-7697-4b10-f7e3-f64328886293"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7b9b556a0710>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extra_trees = FlexibleAIHumanClassifier(classifier_name='extra_trees', cv_folds=5)\n",
        "extra_trees.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl_8rFbnXgR6",
        "outputId": "0e4f78b0-d3eb-4a6c-b240-94b33dfbf9c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9706 +/- 0.0518\n",
            "test_precision: 0.9942 +/- 0.0077\n",
            "test_recall: 0.9471 +/- 0.1053\n",
            "test_f1: 0.9667 +/- 0.0596\n"
          ]
        }
      ],
      "source": [
        "extra_trees_results = extra_trees.get_cv_results()\n",
        "for metric, values in extra_trees_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yE3tKtZx_Tih",
        "outputId": "6cfdac63-3882-4473-e3d8-3e4624efd9ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7b9b56797740>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extra_trees10 = FlexibleAIHumanClassifier(classifier_name='extra_trees', cv_folds=10)\n",
        "extra_trees10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C32cJVol_T4_",
        "outputId": "6de17cb9-f265-43db-a949-2819951b4c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9940 +/- 0.0109\n",
            "test_precision: 0.9954 +/- 0.0090\n",
            "test_recall: 0.9928 +/- 0.0208\n",
            "test_f1: 0.9940 +/- 0.0111\n"
          ]
        }
      ],
      "source": [
        "extra_trees_results10 = extra_trees10.get_cv_results()\n",
        "for metric, values in extra_trees_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTHEqLdOjNFD"
      },
      "source": [
        "**LDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVGhHtVjjRGD",
        "outputId": "504606e7-809e-4549-f84c-6f9c913958d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e6423a421b0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lda = FlexibleAIHumanClassifier(classifier_name='lda', cv_folds=5)\n",
        "lda.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yol-N2NnXsjH",
        "outputId": "74ad3dec-c715-4d78-c00c-74564b7946bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9587 +/- 0.0201\n",
            "test_precision: 0.9432 +/- 0.0219\n",
            "test_recall: 0.9774 +/- 0.0392\n",
            "test_f1: 0.9594 +/- 0.0207\n"
          ]
        }
      ],
      "source": [
        "lda_results = lda.get_cv_results()\n",
        "for metric, values in lda_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur5Slzz4_q_h"
      },
      "outputs": [],
      "source": [
        "lda10 = FlexibleAIHumanClassifier(classifier_name='lda', cv_folds=10)\n",
        "lda10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O_Qyhyg_rY9"
      },
      "outputs": [],
      "source": [
        "lda_results10 = lda10.get_cv_results()\n",
        "for metric, values in lda_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGQF0q5njgZQ"
      },
      "source": [
        "**xgboost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_rYoE7oujhBv",
        "outputId": "7dbd70a1-53f2-4435-8be2-0adb065139d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7e6423a42ba0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgboost = FlexibleAIHumanClassifier(classifier_name='xgboost', cv_folds=5)\n",
        "xgboost.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NPS-k7OQX4oW",
        "outputId": "ff922046-461e-40c0-a418-d269a51bd5b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9745 +/- 0.0170\n",
            "test_precision: 0.9680 +/- 0.0264\n",
            "test_recall: 0.9827 +/- 0.0296\n",
            "test_f1: 0.9748 +/- 0.0169\n"
          ]
        }
      ],
      "source": [
        "xgboost_results = xgboost.get_cv_results()\n",
        "for metric, values in xgboost_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CQen0n9_3p6"
      },
      "outputs": [],
      "source": [
        "xgboost10 = FlexibleAIHumanClassifier(classifier_name='xgboost', cv_folds=10)\n",
        "xgboost10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np-lbdtF_4Yz"
      },
      "outputs": [],
      "source": [
        "xgboost_results10 = xgboost10.get_cv_results()\n",
        "for metric, values in xgboost_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tGjKR45hRg0"
      },
      "source": [
        "***SVM***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo_kOZ_chZpm",
        "outputId": "8ec1246a-eade-4e1f-c458-34434f500ec8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.FlexibleAIHumanClassifier at 0x7c97a9abf740>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svm = FlexibleAIHumanClassifier(classifier_name='svm_linear', cv_folds=5)\n",
        "svm.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojYXPxqNYFI4",
        "outputId": "9a63d902-3de6-49b2-c3fe-133ea405c350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_accuracy: 0.9736 +/- 0.0139\n",
            "test_precision: 0.9637 +/- 0.0212\n",
            "test_recall: 0.9851 +/- 0.0286\n",
            "test_f1: 0.9739 +/- 0.0140\n"
          ]
        }
      ],
      "source": [
        "svm_results = svm.get_cv_results()\n",
        "for metric, values in svm_results.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Sre5gtNAC6e"
      },
      "outputs": [],
      "source": [
        "svm10 = FlexibleAIHumanClassifier(classifier_name='svm_linear', cv_folds=10)\n",
        "svm10.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axHNziaEADSC"
      },
      "outputs": [],
      "source": [
        "svm_results10 = svm10.get_cv_results()\n",
        "for metric, values in svm_results10.items():\n",
        "    print(f\"{metric}: {values['mean']:.4f} +/- {values['std']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BwzW9fL_rHHH",
        "7TwlxgNpr6sW",
        "V0CAm13nsEIs",
        "CeqG8ysIsZIh"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNgRqiyUbpannWXBAsvUsWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}